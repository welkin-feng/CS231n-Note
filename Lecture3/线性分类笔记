课时 7-8

上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。
我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的
图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：
- 分类器必须记住所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。
- 对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。

我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方
法主要有两部分组成：一个是评分函数（score function），它是原始图像数据到类别分值的映射。另一个是损失
函数（loss function），它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化
问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。

线性分类器：在本模型中，我们从最简单的概率函数开始，一个线性映射：
f(x_i, W, b) = W·x_i + b
参数W被称为权重（weights）。b被称为偏差向量（bias vector），这是因为它影响输出数值，但是并不和原始数据x_i产生关联。

损失函数：
1. SVM 损失函数
Li = ∑(j ≠ y_i) max(0, s_j - s_y_i + Δ)
s = f(x_i, W, b) ，得到不同分类类别的分值
s_j 表示第j个类别的得分
Δ 是超参数，表示阈值

2. softmax 损失函数
Li = - log(e^s_y_i / ∑(j)e^s_j)


正则化：
使损失函数最小的 W 不唯一，所以要在损失函数中增加一个正则化部分，来得到我们想要的 W
L2正则化：W 中每个元素的平方和。使W不仅能正确分类，还可以按照我们想要的方式尽可能的分散。
L1正则化：W 中每个元素的绝对值之和。会使W中的数据尽可能稀疏，W 中会出现很多0
有研究结果说明为什么正则化是个很好的应用，对于这门课，L2正则化会使测试结果更好。
L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数
几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免过拟合。


最终损失函数：L = 1/N·∑(i=1, N)∑(j≠y_i)[max(0, f(x_i; W)_j - f(x_i; W)_y_i + Δ) + λ·R(W)

精确地说，SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。
Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得
来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意
从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这
个说法常常被用来做简称。



梯度下降：
每次从训练集中选取一个mini-batch来判断梯度，然后重复。不过由于每次只取了很小一部分数据，所以得到的梯度可能是没有意义的噪声。
但是取样算梯度可以提高运算速度，从而可以多次计算得到最低损失函数。
也可以减少循环次数，使用更精确的梯度进行计算。但是实际操作中，mini-batch的效率更高。而且使用所有数据来进行每一步梯度下降算法的循环也是不切实际的。
通常mini-batch的大小为32/64/256，这样就不用担心GPU不能承受原来那么大的计算量了

数值梯度：写起来容易，但是计算缓慢
解析梯度：需要用微积分计算，可以很快得到结果，但是结果容易出错，因为要解决很难的数学问题

用微积分计算梯度的值，用数值梯度的方法检验结果是否正确