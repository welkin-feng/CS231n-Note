课时 13-14

梯度检查
	1. Mini-batch SGD
		x += - learning_rate * dx

	2. Momentum update
		v = mu * v - learning_rate * dx
		x += v

	3. Nesterov Momentum update (nag)
		v_prev = v
		v = mu * v - learning_rate * dx
		x += - mu * v_prev + (1 + mu) * v

	4. AdaGrad update
		cache += dx ** 2
		x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)

	5. RMSProp update
		cache = decay_rate * cache + (1 - decay_rate) * dx ** 2
		x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)

	6. Adam update
		m = beta1 * m + (1 - beta1) * dx
		v = beta2 * v + (1 - beta2) * (dx ** 2)
		x += - learning_rate * m / (np.sqrt(v) + 1e-7)


Dropout