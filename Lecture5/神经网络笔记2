课时 12

一、 数据预处理：
	1. 中心化处理、均值减法（基本上所有数据都会用到）
	  对于图像处理，可以每个数据减去所以数据的均值，也可以减去RGB单通道的均值
	  X -= np.mean(X, axis=0)

	2. 归一化处理（每个维度除以标准差，变成分布在[-1, 1]之间的数据）
	  通常用于将不同维度上的特征用统一的标准度量。在图形处理中不常用，因为所有维度都是0-255
	  X /= np.std(X, axis=0)

	3. PCA（主成分分析，用于数据降维）
	  数据存在协方差，PCA算法可以把协方差矩阵变成对角矩阵

	  # 假设输入数据矩阵X的尺寸为[N x D]
	  X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)
	  cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵
	  U,S,V = np.linalg.svd(cov) # U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）

	  # 为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上
	  Xrot = np.dot(X,U) # 对数据去相关性

	  注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准
	  正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如
	  果计算Xrot的协方差矩阵，将会看到它是对角对称的。np.linalg.svd的一个良好性质是
	  在它的返回值U中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降
	  维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也
	  被称为主成分分析（ Principal Component Analysis 简称PCA）降维：
	  Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100]

	4. 白化
	  在PCA处理之后，对数据进行压缩，将协方差矩阵变成单位矩阵

	  白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。
	  该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是
	  一个均值为零，且协方差相等的矩阵。该操作的代码如下：

	  # 对数据进行白化操作:
	  # 除以特征值
	  Xwhite = Xrot / np.sqrt(S + 1e-5)

	------归一化、PCA和白化在图像处理中没有应用，在机器学习中很常见------

	5. 常见错误：
	  进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计
	  算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均
	  值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错
	  误的。应该怎么做呢？

	  应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试
	  集）中的图像再减去这个平均值。


二、 权重初始化：
	使用何种初始化方法是数据驱动的，使用小批量数据通过2-3层网络，观察每层输出数据的方差，我们希望数据是高斯分布

	对于sigmoid激活函数：
		1. 小的随机数（使用标准差为0.01的高斯分布随机数）
		  对于层数较少的神经网络效果很好
		  W = 0.01 * np.random.randn(D,H)

		2. 使用1/sqrt(n)校准方差
		  np.random.randn(out_num, in_num) / np.sqrt(in_num)

		  对于ReLU激活函数：
		  np.random.randn(out_num, in_num) / np.sqrt(in_num / 2)

		3. 偏置（biases）的初始化
		  通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性

		4. 批量归一化
		  X = (X - EX) / √Var(X)
		  在神经网络的每一层的全连接和激活函数之间，即对 W * X 的输出进行归一化。
		  不过对于ReLU激活函数，这样会使输出变得饱和，所以通常在归一化后对 X 进行移动，
		  X = gamma * X + beta，其中 gamma 和 beta 是可以学习的参数，
		  如果设置成 EX 和 Var(X)（归一化之前的均值和方差），则相当于对X做了一次恒等变换。
		  在测试之前，EX 和 Var(X) 应该被固定下来，在测试时直接使用。可以在训练时将这些参
		  数记录下来，使用其均值，或者使用整个训练集的EX和Var(X)作为参数


三、 正则化：
	1. L2正则化
	  对于网络中的每个权重w，向目标函数中增加一个 0.5 * lambda * w^2，其中 lambda是正则化强度
	  使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降

	2. L1正则化
	  对于每个w我们都向目标函数增加一个 lambda * |w|

	3. 随机失活
	  在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0

	    """
	    反向随机失活: 推荐实现方式.
	    在训练的时候drop和调整数值范围，测试时不做任何事.
	    """

	    p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱

	    def train_step(X):
	      # 3层neural network的前向传播
	      H1 = np.maximum(0, np.dot(W1, X) + b1)
	      U1 = (np.random.rand(*H1.shape) < p) / p # 第一个随机失活遮罩. 注意/p!
	      H1 *= U1 # drop!
	      H2 = np.maximum(0, np.dot(W2, H1) + b2)
	      U2 = (np.random.rand(*H2.shape) < p) / p # 第二个随机失活遮罩. 注意/p!
	      H2 *= U2 # drop!
	      out = np.dot(W3, H2) + b3

	      # 反向传播:计算梯度... (略)
	      # 进行参数更新... (略)

	    def predict(X):
	      # 前向传播时模型集成
	      H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了
	      H2 = np.maximum(0, np.dot(W2, H1) + b2)
	      out = np.dot(W3, H2) + b3

-----通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同
时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参--------


四、 损失函数
	1. L2损失
	  L_i = ||f-y_i||^2_2

	2. Softmax损失
	  L_i = -log( e^f_yi / ∑ e^f_j)


总结：
1. 推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化
   到[-1,1]范围之内。
2. 使用标准差为√{2/n}的高斯分布来初始化权重，其中n是输入的神经元数。例如用
   numpy可以写作：w = np.random.randn(n) * sqrt(2.0/n)。
3. 使用L2正则化和随机失活的倒置版本。
4. 使用批量归一化。
