课时10

激活函数：
sigmoid:

sigmoid = 1/(1 + exp(-x))
dσ/dx = σ * (1 - σ)
dL/dx = dσ/dx * dL/dσ = σ * (1 - σ) * dl/dσ
问题：
(1)一个饱和的神经元（接近0或者1），在反向传播中梯度趋于0，梯度消失
当输入的神经元数据全正或全负时，梯度要么全正，要么全负，所以得到的梯度下降曲线将很曲折，而不是沿着最快的方向直线下降。
(2)计算比较耗时
(3)sigmoid 不是关于原点对称的


tanh
优点：
0中心
问题：
神经元饱和的时候梯度依旧会消失


ReLU => max(0, x)
优点：
(1)不饱和
(2)计算得快
(3)收敛速度比 sigmoid/tanh 更快
问题：
(1)不关于原点对称
(2)如果神经元的输出数据不存在，那么他的梯度将会消失，这个神经元就失活了


Leaky ReLU => max(0.01x, x)
Maxout
ELU
