课时 5-6

最邻近算法：将测试数据与训练数据中的每一项一一比对，找到所有点的距离之和最小的那一个作为此测试数据的分类。
k最邻近算法：找到k个最邻近的数据，然后将这k个数据中出现的最多的分类作为这个测试数据的分类
k是超参数(hyperparameters)
选择k的方法：使用验证数据集，即将测试数据集分为几份，选取其中一份作为验证数据集。
    取不同的k值训练另外的数据集，每次训练后使用验证数据集进行准确率验证，最后选择准确率最高的k。

NN分类器能够在CIFAR-10上得到将近40%的准确率。
NN分类器的优点：
1. 易于理解，实现简单
2. 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来

NN分类器的缺点：
1. 需要存储所有训练数据
2. 测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，而且时间随数据量增加而线性增加
3. 将同一张图片进行平移后，距离会变大，无法识别


我们需要的算法应该在训练完成后，单一数据的测试时间不会随数据量的增加而变化.

超参数：
k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？
我们可以选择不同的距离函数，比如L1范数和L2范数等，那么选哪个好？
还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为超参数（hyperparameter）。
在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。

选取超参数的正确方法是：
将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。
如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。
一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。

小结：实际应用k-NN

如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：

1. 预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。
   在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。
2. 如果数据是高维数据，考虑使用降维方法，比如PCA(wiki ref, CS229ref, blog ref)或随机投影。
3. 将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。
   这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。
   如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。
   如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。
4. 在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式。
5. 如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如FLANN）来加速这个过程，其代价是降低一些准确率。
6. 对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？
   因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。
   在实践中，不要这样做。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。
   直接使用测试集来测试用最优参数设置好的最优模型，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。
