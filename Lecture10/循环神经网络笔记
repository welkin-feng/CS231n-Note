课时 21-22

Recurrent Neural Networks

优势: 在建立神经网络时有很高的灵活性

- 普通的神经网络是1对1的，输入固定大小的图像，输出固定大小的向量，即class score.
- 在RNN中可以采用不同的顺序实现，比如从输入开始或从输出开始，或两者同时开始.
	1对多: 图像字幕(图像->词的序列)，输入一个固定大小的图像，通过RNN会生成一系列按顺序排列的描述图像内容的词，这些词形成一句话就是这幅图的描述.
	多对1: 情感分类(词的序列->情感)，处理一定数量的按顺序排列的词，然后把句子中的词按正/负面感情来分类.
	多对多(多个中间层节点): 语言翻译(词的序列->词的序列)，把一些单词翻译成另一种语言的一系列单词(从一个序列翻译至另一个序列).
	多对多: 基于帧的视频分类，将视频中每一帧按一定数量的类来分类，并且每一帧还会利用前后帧的状态.


RNN有一个状态，可以随时间变化接收变量，我们可以用一个循环公式在每一时间步长处理一系列向量x:
	h[t] = fw(h[t-1], x[t])
	其中h[t]是新状态，fw是含有参数w递归的函数，h[t-1]是旧状态，x[t]是在某一时刻的输入向量.


普通RNN:
	x -> RNN -> y
	h[t] = fw(h[t-1], x[t]) ->
	h[t] = tanh(w_hh * h[t-1], w_xh * x[t])
	y[t] = w_hy * h[t]


应用
	1. 字符级语言模型(character- level language model)
		工作原理: 把一系列字符输入到RNN中，在每一个时间步长中，我们要求RNN预测下一个字符是什么. 所以它会根据所看到的所有字符来预测下一个字符.
		例如，现有字符集 [h, e, l, o] 和一个训练序列 "hello"，我们试着在这组数据中利用RNN来学习预测序列下一个字符.

	2. 图像描述(image caption)
		在计算机视觉的条件下，用RNN做图像描述. 输入一张图片，然后用一些词语进行描述.
		整个模块由两部分组成，CNN负责处理图像，RNN负责建立模型.
			- 将一张图片放入CNN网络中(例如vgg16)，删除CNN中的softmax分类器，将fc层直接接上RNN.
			- 正常的RNN隐藏层是tanh(Wxh * x + Whh * h)，现在需要为RNN增加额外的条件，额外增加一项 Wih * v，v指卷积网络的输出.
				h = tanh(Wxh * x + Whh * h + Wih * v) (有多种方法将图片信息连接到RNN，这只是其中一种，而且图像信息通常只加入到RNN的第一步中)
			- RNN会再最开始接收来自输入的信息x0 = "<START>"，初始隐藏状态h0 = 0，和来自卷积网络输出的图像信息v0，然后计算后得到第一个预测词y0.
				接下来会将y0作为x1输入RNN，Whh * h0作为h1，依此循环，直到得到预测词为"<end>".


复杂RNN:
	使RNN变复杂的方法有很多.

	1. 增加层数
		一般情况下，层数越多工作效果越好. 堆叠层数的方法很多.
		- 增加RNN隐藏层层数，下一个RNN的输入，是前一个RNN的隐藏状态，每一个隐藏层拥有独立的权重.


	2. 使用复杂的公式——LSTM(Long Short Term Memory)
		LSTM方程:
			通常的RNN每步只有一个h向量，LSTM每次有两个向量，h(hidden vector) 和 c(cell state vector)，其基本工作就是对c进行操作.
			1. 基本方程:
				  W * (x, h).T
				 其中 W 为[4n x 2n]维，x是输入，h是前一个隐藏层状态，x和h大小都是n维(由n个数字组成)，方程会产生一个4n维向量.
			2. i, f, o, g方程:
				  (i, f, o, g) = (sigm, sigm, sigm, tanh)(W * (x[t], h[t-1]).T)
				 4n个数字分成4组，分别通过i(input), f(forget), o(output), g四个门(其中i, f, o是sigmoid，g是tanh)，得到4个n维向量.
				 i, f, o, g实现了对c的操作，把i, f, o想象成二进制门，输出0或1，对它们进行sigmoid是为了使其可微分.
				 通常会在f中加入偏差bias，使forget门在一开始处于关闭状态，使一开始的梯度传播非常顺利.

			3. c, h方程:
				  c[t] = f * c[t-1] + i * g
				  h[t] = o * tanh(c[t])
				 其中*代表数组乘法运算. h[t]会被用于预测或作为下一隐藏层的输入，和传递到下一时间步长的计算中.
				 使用 i * g是为了使cell变得更复杂，g表示要在细胞状态里增加多少，i表示是否想要增加.

		- LSTM与ResNet很像，forget门类似于残差结构，所以LSTM不会出现梯度消失的问题.
		- output门并不是很重要，有一篇论文LSTM: A Search Space Odyssey对各种门做了大量研究.

		GRU方程(LSTM的一种变化形式):
			r[t] = sigm(Wxr * x[t] + Whr * h[t-1] + br)
			z[t] = sigm(Wxz * x[t] + Whz * h[t-1] + bz)
			h_tmp[t] = sigm(Wxh * x[t] + Whh * (r[t].T * h[t-1]) + bh)
			h[t] = z[t].T * h[t-1] + (1 - z[t]).T * h_tmp[t]

			- GRU的公式更简短，它只有一个h向量，没有c向量


总结:
	- RNN有许多架构设计上的灵活性.
	- Vanilla RNN很简单，但效果不是很好.
	- 通常使用LSTM或GRU来代替: 它们的加法运算使梯度传播更容易.
	- RNN的梯度反向传播存在梯度爆炸和梯度消失问题.
		梯度爆炸可以使用梯度裁剪来解决. 梯度消失可以使用加法运算(LSTM)来控制.
	- 简单好用的结构是当前研究的热点.
	- 在经验和理论上都需要更深刻的理解. 因为残差和LSTM之间的联系是很复杂的，我们还没有完全理解为什么它们效果很好.